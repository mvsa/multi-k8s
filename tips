docker-compose up
Original Files/folders deleted: travis.yml / docker-compose / dockerrun.aws / ./nginx
Folder k8s created to house all kubenets configs
created multi client config
created clusterIp service attached to multi clients
created multi server (express) config
created clusterIp service attached to multi server
created multi worker config file
created redis config file and its service
created postgres config file and its service
created claim config files and updated postgres deployment
defined env variable to multi worker(redishost/redisport)
defined env variable to multi server (pguser, pghost, pgdatabase, pgport, pgpassword)
installed ingress specific for docker desktop windows/ and GCE-GKE load balancer
created ingress config file
--------------------------------------------------------------------------------------
This is a multi container application that originally was running on elastic beanstalk and will be converted to
kubernetes (1 config file for each object = total of 11)


---------------------------------------------------------------------------------------------------------------
**Persistent volume claim** 
PVC
Volume: share file system of a host machine with file system inside a container

if a pod crashes/restarts, and inside this pod we have a postgree container, the data stored inside this pod will be lost! (because the crashed pod will be deleted and a new one created)

by sharing the system file of a host to the containers it gives the pod the capacity to store some volume of data on the host that its running (good for databases)

only increasing the replicas (number) of a postgres deployment will not be the best strategy because we will end up with two pods accessing the same volume.
The problem is the pods is not aware of each other and it can cause problems.
To scale databases we needo to follow some additional configs besides just incrementing the number of replicas


Volume in generic container terminology = some type of mechanism that allows a container to access a filesystem outside itself

Volume in Kubernets = an OBJECT that allows a container to store data at the POD level {Persistent volume claim *** / Persistent volume ** / Volume*}

* Volume in kubernets will survive container restarts, but not pod restarts, not good for databases


** Persistent Volume: alowws container to store data outside the pod/deployment.
It can survive restarts and crashes

** Persistent Volume Claim: 
advertisiment of options of storage given to be used in the future, there are two options: 

1- Staticlly provisioned persistente volume = volumes available ahead of time, they have alread been created and can be used

2 - Dinamically provisioned persisten volume = volumes that are not yet been created but can be created on the fly when requested.

We will attach this PVC to the postegres deployment

in a dev enviroment the only option to storage available will be the harddrive of the device, and so we dont need to specify the source in the config file
In an cloud provider (prod) kubernets will automaticly set based on wich storage option you are using
but i can me manually set too (storageclassname)

known storage options: {
    Google cloud persistent disk / azure file / azure disk / aws block store
    kubernetes.io/docs/concepts/storage/storage-classes
}

pvc = is the advertisment of what can be get
pv = is the actual instance of storage that meets the requirements of the pvc


**ClusterIP Service**
Exposes a set of pods to other objects in the cluster
Allow that any other object inside our cluster to access the object that the clusterip is point out. Only internal
not to the outside world

In ex, if a deployment has a clusterIP atached to it, this deployment can be accessed by any other object inside the cluster. Whitou it they will become unreachable

**NodePort Service**
Exposes a container to the outside world (only good for dev purposes)

**Ingress Service**
Front door, traffic (outside world) will come to the ingress and then the deployments will be accessible
Exposes a set of services to the outside world
There are some differente implementation of ingress, in this project we are using Nginx Ingress
 - We are using ingress-nginx, a community led project: github/kubernets/ingress-nginx
 - We are not using kubernetes-ingress, a project led by the company nginx: github/nginxinc/kubernetes-ingress

The setup of our ingress-nginx changes depending on our enviroment (loca, GC, aws, azure, etc)
In this projetc, it will be setup in local and GC


In ingress we also use the concept of 'Controller' (see Trivia section).
Where we have a desired state described in a file (ex: routing rules to get traffic to services)
and we feed that file in kubclt and a controler is create to make our current state to change to our desired state (in this case, current state were 'no routing' and new state 'pod running nginx that handles routing, and is always waching for changes or updates' )

Ingress config > ingress controller > something that accepts inconming traffic and redirect to apropriate service

obs: with the project used (ingress-nginx) the controller and the 'something' that routes traffic are the same entity (we will be a single deployment)


(check images, on GC the loadBalancer service will still be used, but in a encapsulated manner)
( check images, when we use a nginx ingress there is going to be another deployment inside our cluster named
default-backend that is used for a series of health checks in the cluster, in a ideal case, this deployment is replaced by our express api server,  it is ideal that helth checks goes to the multi server, but in this project we will be using the defaul-backend)

 - q: Why we just dont use a normal nginx pod to make this traffic handle?
    a: we will ingress-nginx because it has many pieces of code that makes it aware that is running inside a kubernets cluster. An example is it can bypass the clusterIP (evading a random load balanced access) service in front of a deployment, and directly access the pods. It allows features like 'Sticky sessions' that make possible that all requests from a user goes directly to the same server

further reading about ingress-nginx: https://www.joyfulbikeshedding.com/blog/2018-03-26-studying-the-kubernetes-ingress-system.html

Ex of traffic routing:
Look at the path of incoming request -> if has a path of / -> goes to client
                                    -> if has a path of /api goes to server




**Load Balance Service** 
Legacy way of getting network traffic into a cluster, only grants access to ONE set of pods
it will use your cloud provider (aws, google, etc) and will create a loadBalancer with their configuration or definition of what a load balance is



**Secrets**
Object Type that securely stores a piece of information in the cluster, such as
database password
to create a secret we use a imperative command,this way we dont have to write our secret in some config file.
This means that in a production enviroment we will have to manually set this config aswell.

- kubectl create secret generic <secrete_name> --from-literal key=value -
    ex kubectl create secret generic pgpassword --from-literal PGPASSWORD=pass123

generic = type Of secret {other types of types docker-registry / tls}
--from-literal = means that we are going to add the secret information into this command, as opposed to from a file

-------------------------------------------------------------------------------------
**COMMANDS**
kubectl apply -f k8s   =  apply all config files from one directory
kubectl logs <name of object>

kubectl get storageclass = return the list of available storage options
kubectl describe storageclass

kubectl create secret generic <secrete_name> --from-literal key=value

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.0/deploy/static/provider/cloud/deploy.yaml

kubectl get pods -n ingress-nginx


-------------------------------------------------------------------------------------------
**TRIVIA:**
Nginx server = routing inside application was replaced by ingress service

rather than putting a separate file for each one of the objects we can create a single file to group different objects config, just copy and paste on the end of each config followed by a --- at the final line (not doing this in this project)

When aplying config giles = cannot convert int64 to string
occours when in our config files we provided some env variable as integers:
ex: value:6579
to correct this, use single quotes on this values 

In kubernets a controller is any kind of object that constantly works to make some desire state a reality inside our cluster.
ex. We have a config file that set our spectation from a desired state, then a deployment is mande to make that we get out from a current stat of 0 running pods to a new state of X running pods based on our config file

replicasets and replication controlers are deprecated in order of Deployments

**ADMIN DASHBOARD**
get most recente script: https://github.com/kubernetes/dashboard#install

Open up the downloaded file in your code editor and use CMD+F or CTL+F to find the args. Add the following two lines underneath --auto-generate-certificates:

args:
  - --auto-generate-certificates
  - --enable-skip-login
  - --disable-settings-authorizer



   Run the following command inside the directory where you downloaded the dashboard manifest file a few steps ago:

kubectl apply -f kubernetes-dashboard.yaml

 Start the server by running the following command:

kubectl proxy

 You can now access the dashboard by visiting:

http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

Click the "SKIP" link next to the SIGN IN button.

! The only reason we are bypassing RBAC Authorization to access the Kubernetes Dashboard is that we are running our cluster locally. You would never do this on a public-facing server like Digital Ocean and would need to refer to the official docs to get the dashboard setup.

If you wish to instead create a sample user, you can follow the instructions here:

https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md



---------------------------------------------------------------------------------------
Process of deploy:
Create github repo > 
Tie repo to Travis CL **
Create Google Cloud Project
Enable billing *
add deployment scripts

*Google cloud cost calculator  = kubernets engine = 3 nodes = standart = 1 persist disk 2gb
load balance 1 forwarding rule > 100 mbnetword traffic proccedd :
cost: 42 dolar per month
1,40 cents a day

Why Google Cloud:
    Google created Kubernetes
    AWS only 'recently' got kubernetes support
    far easier to poke around k8s on GC
    Good documentation for begginers


1 - ** https://app.travis-ci.com/{
create account
Sync repos of github

}

2 - Create google cloud project > enable billing to it
then, menu > Kubernets Engine >  Enable > after enabled click on create'> Configure GKE Standard:
Name: multi cluster (name of projetc)
zone: southamerica
default-pool dropdown: machine specifcs
After a few minutos check the cluster dashboard
and click on the cluster name to see its specifications

Menu workloads= lists pods and deployments
configuration = lists secretes or env. variables

3 - Travis ymal file, builds images and deploy application to gke

config file general idea:
install google cloud sdk cli - cli that allows us to interact with kubernetes cluster in GKE
configure the sdk with out google cloud auth info
login to docker cli
build the test version of multi-client
run testes
if testes are sucessful, run a script to deploy newest images
build all our images, tag each one, push each to docker hub
apply all configs in the k8s folder
imperatively set latest images on each deployment

create .travis.yaml file inside the root of project

## to create a service account .json file
go for IAM & Admin > service account > create service account
name: travis-deployer (could be any other) > create and continue
Select role > Kubernets Engine > Kubernetes engine admin > continue > one
-In the list of service account, click on the three dots > Manage Keys

Add Key> create a new key>Json (DO NOT EXPOSE THIS FILE)
>Download and install the travis CLI ##
> Encrypt and upload the json file to our travis account
> in travis.yml, add code to unencrypt the json file and load it into Gcloud SDK

## Installin travis CLI
It requires ruby to be installed locally, and because of that it will be better to use a docker image that has ruby
and acces its shell to isntall travis cli. Because we will just use the travis clt o encrypt the
file and after that we can delete it (the container)

> winpty docker run -it -v ${pwd}:/app ruby:2.4 sh -- for gitbash i can iuse ()/ prefer to run it in powershell
{
    -v ${pwd}:/app = here we are setting up an volume to be able to store de .json file in it
    pwd = present working directory
    :/app = pwd will be a directory named app
    its important to run this command in the folder of the project, because we will then get access to our folder project inside the container inside the folder app

    inside the container
    ls > cd app
}

> gem install travis
>travis login {

The Travis login now requires a Github Token. Please follow these instructions to create a Personal Token for Travis to use here:

https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/creating-a-personal-access-token

This will also require setting the scope. Travis requires the permissions noted here:

https://docs.travis-ci.com/user/github-oauth-scopes/#repositories-on-httpstravis-cicom-private-and-public

The login command will now look like this:


travis login --github-token YOUR_PERSONAL_TOKEN --com

or

travis login --github-token YOUR_PERSONAL_TOKEN --pro

When you encrypt the file, you must pass the same --com or --pro flag you used to log in:

travis encrypt-file service-account.json -r USERNAME/REPO --com

or

travis encrypt-file service-account.json -r USERNAME/REPO --pro

If you are getting iv undefined errors, you may have missed passing the --com or --pro flags to both the login and encryption commands. Also, if you still have a .org Travis account these old accounts will need to be migrated to .com ASAP.

Please visit the migration guide here:

https://docs.travis-ci.com/user/migrate/open-source-repository-migration#migrating-a-repository

You can also get an iv undefined error if you've passed the wrong repo to the file encryption or passed a repo name with a typo in it. Please note, after the migration, or after fixing a typo, you'll need to run through the entire encryption process again.


}
>copy and rename json file in the 'volumed' directory so wa can use it in the container
>travis encrypt-file service-account.json -r mvsa/multi-k8s --com
> update yaml files with instrucins of travis (and get a file that will be created)
> DELETE THE ORIGINAL json file
> after that you can remove de container travis